#!/usr/bin/env python3
"""
generate_overlay.py — Reconfigurable LLNN Overlay Generator

Forked from convert2sv.py. Instead of baking truth tables into behavioral
case statements, this script emits SoftLUT5 (CFGLUT5) instances whose
INIT values are programmed at runtime via an AXI-Lite interface.

The connectivity graph (DAG) is PRESERVED identically from the trained
PyTorch model — only the *function* of each gate becomes dynamic.

Usage:
    python generate_overlay.py --model simple_lutnn [--name my_overlay]
"""

import sys
import os
import argparse
import json
import shutil
import torch
from pathlib import Path
import numpy as np

from torch.nn import Flatten
import torch.nn.functional as F

sys.path.append(str(Path(__file__).parent.parent))
from lutnn.lutlayer import LUTLayer, Aggregation


# =============================================================================
#  CLI
# =============================================================================

def get_args():
    parser = argparse.ArgumentParser(
        description="Generate a reconfigurable LLNN overlay (SoftLUT5 + AXI)."
    )
    parser.add_argument("--model", type=str, required=True,
                        help="Model name (stem of .pth file in models/)")
    parser.add_argument("--name", type=str,
                        help="Output folder name (defaults to --model)")
    return parser.parse_args()


# =============================================================================
#  Model parameter extraction  (unchanged from convert2sv.py)
# =============================================================================

def get_model_params(model):
    """Extract network dimensions from the trained model."""
    lut_size = []
    num_neurons = []
    number_of_inputs = -1
    for layer in model.model:
        if isinstance(layer, LUTLayer):
            if number_of_inputs == -1:
                number_of_inputs = torch.max(layer.indices).item() + 1
            lut_size.append(layer.indices.shape[0])
            num_neurons.append(layer.indices.shape[1])
        elif isinstance(layer, Aggregation):
            number_of_classes = layer.num_classes
    number_of_layers = len(num_neurons)
    return number_of_layers, num_neurons, lut_size, number_of_inputs, number_of_classes


def get_wiring(model, lut_size):
    """
    Extract ONLY the connectivity graph (indices) from each LUTLayer.
    Unlike get_net_layers() in convert2sv.py, this does NOT extract truth tables.
    The truth tables will be loaded at runtime via AXI.
    """
    layers = []
    idx = -1
    for layer in model.model:
        if isinstance(layer, LUTLayer):
            idx += 1
            layers.append(layer.indices)  # shape: [lut_size, n_neurons]
        elif isinstance(layer, (Flatten, Aggregation)):
            pass
        else:
            assert False, f'Unknown layer type: {type(layer)}'
    return layers


# =============================================================================
#  File generation helpers
# =============================================================================

def create_folder(path):
    Path(path).mkdir(parents=True, exist_ok=True)


def gen_globals_file(sv_path, number_of_inputs, number_of_layers,
                     num_neurons, lut_size, outputs_per_class,
                     output_bits, total_gates):
    """Generate Globals.sv with network parameters."""
    with open(os.path.join(sv_path, "Globals.sv"), "w") as f:
        f.write("// Auto-generated by generate_overlay.py\n")
        f.write("// DO NOT EDIT — regenerate from the trained model instead.\n\n")
        f.write("`ifndef GLOBALS_SV\n`define GLOBALS_SV\n\n")
        f.write(f"localparam NET_INPUTS     = {number_of_inputs};\n")
        for i in range(number_of_layers):
            f.write(f"localparam L{i}_NEURONS    = {num_neurons[i]};\n")
        f.write(f"localparam CLASS_OUTS     = {outputs_per_class};\n")
        f.write(f"localparam NET_OUTPUT_BITS = {output_bits};\n")
        f.write(f"localparam TOTAL_GATES    = {total_gates};\n")

        # Per-layer gate base offsets
        base = 0
        for i in range(number_of_layers):
            f.write(f"localparam L{i}_GATE_BASE = {base};\n")
            base += num_neurons[i]

        f.write(f"localparam GATE_SEL_W     = $clog2(TOTAL_GATES);\n")
        f.write("\n`endif // GLOBALS_SV\n")


def gen_comparator_file(sv_path):
    """Generate comparator.sv — identical to convert2sv.py output."""
    with open(os.path.join(sv_path, "comparator.sv"), "w") as f:
        f.write("// Auto-generated by generate_overlay.py\n\n")
        f.write("`include \"Globals.sv\"\n\n")
        f.write("module comparator (\n")
        f.write("\tinput  logic [$clog2(CLASS_OUTS+1)-1:0] in1,\n")
        f.write("\tinput  logic [$clog2(CLASS_OUTS+1)-1:0] in2,\n")
        f.write("\toutput logic [$clog2(CLASS_OUTS+1)-1:0] max,\n")
        f.write("\toutput logic                            idx\n")
        f.write(");\n\n")
        f.write("\talways_comb begin\n")
        f.write("\t\tif (in1 >= in2) begin\n")
        f.write("\t\t\tidx = 1'b0;\n")
        f.write("\t\t\tmax = in1;\n")
        f.write("\t\tend else begin\n")
        f.write("\t\t\tidx = 1'b1;\n")
        f.write("\t\t\tmax = in2;\n")
        f.write("\t\tend\n")
        f.write("\tend\n\n")
        f.write("endmodule : comparator\n")


# =============================================================================
#  Layer file generation — THE KEY CHANGE
# =============================================================================

def gen_layer_file(sv_path, layer_idx, indices, num_neurons_layer,
                   lut_size_layer, gate_base, total_gates,
                   input_width_param):
    """
    Generate layer{N}.sv with SoftLUT5 instances instead of static case statements.

    Each neuron becomes:
      1. Static wiring: assign statements connecting inputs (PRESERVED from model)
      2. SoftLUT5 instance: CFGLUT5-backed reconfigurable LUT

    The per-gate CE decode is done inside the layer using the global cfg_gate_sel.
    """
    fname = os.path.join(sv_path, f"layer{layer_idx}.sv")
    with open(fname, "w") as f:
        f.write(f"// Auto-generated by generate_overlay.py\n")
        f.write(f"// Layer {layer_idx}: {num_neurons_layer} neurons, "
                f"LUT{lut_size_layer}, gate IDs {gate_base}–"
                f"{gate_base + num_neurons_layer - 1}\n\n")

        f.write("`include \"Globals.sv\"\n\n")

        # Module header with config ports
        f.write(f"module layer{layer_idx} (\n")
        f.write(f"\tinput  logic                    clk,\n")
        f.write(f"\tinput  logic [{input_width_param}-1:0] in,\n")
        f.write(f"\toutput logic [L{layer_idx}_NEURONS-1:0] out,\n")
        f.write(f"\t// Configuration interface\n")
        f.write(f"\tinput  logic [GATE_SEL_W-1:0]   cfg_gate_sel,\n")
        f.write(f"\tinput  logic                    cfg_ce,\n")
        f.write(f"\tinput  logic                    cfg_data\n")
        f.write(f");\n\n")

        # Per-neuron CE decode
        f.write(f"\t// Per-neuron configuration enable decode\n")
        f.write(f"\twire [L{layer_idx}_NEURONS-1:0] local_ce;\n")
        f.write(f"\tgenvar g;\n")
        f.write(f"\tgenerate\n")
        f.write(f"\t\tfor (g = 0; g < L{layer_idx}_NEURONS; g = g + 1) "
                f"begin : ce_decode\n")
        f.write(f"\t\t\tassign local_ce[g] = cfg_ce & "
                f"(cfg_gate_sel == L{layer_idx}_GATE_BASE + g);\n")
        f.write(f"\t\tend\n")
        f.write(f"\tendgenerate\n\n")

        # Instantiate each neuron
        for neuron in range(num_neurons_layer):
            gate_id = gate_base + neuron
            f.write(f"\t// ---- Neuron {neuron}  (Gate ID: {gate_id}) ----\n")

            # Input wiring (PRESERVED from trained model)
            f.write(f"\tlogic [{lut_size_layer-1}:0] lut_in_{neuron};\n")
            for i in range(lut_size_layer):
                src = int(indices[i][neuron])
                f.write(f"\tassign lut_in_{neuron}[{lut_size_layer-i-1}] "
                        f"= in[{src}];\n")

            # SoftLUT5 instance (CHANGED — was case statement)
            pad_bits = 5 - lut_size_layer
            if pad_bits > 0:
                lut_in_expr = f"{{{pad_bits}'b0, lut_in_{neuron}}}"
            else:
                lut_in_expr = f"lut_in_{neuron}"

            f.write(f"\tSoftLUT5 slut_{neuron} (\n")
            f.write(f"\t\t.clk      (clk),\n")
            f.write(f"\t\t.lut_in   ({lut_in_expr}),\n")
            f.write(f"\t\t.lut_out  (out[{neuron}]),\n")
            f.write(f"\t\t.cfg_ce   (local_ce[{neuron}]),\n")
            f.write(f"\t\t.cfg_data (cfg_data),\n")
            f.write(f"\t\t.cfg_out  ()  // not daisy-chained\n")
            f.write(f"\t);\n\n")

        f.write(f"endmodule : layer{layer_idx}\n")


# =============================================================================
#  Top module generation
# =============================================================================

def print_idx(f, comparator, n_idx):
    """Argmax encoding logic — identical to convert2sv.py."""
    for i in range(n_idx - 1):
        if comparator == 0:
            if i == (n_idx - 2):
                f.write(f"idx{i} == 1'b0")
            else:
                f.write(f"idx{i} == 1'b0 && ")
        else:
            if i == (comparator - 1):
                if i < (n_idx - 2):
                    f.write(f"idx{i} == 1'b1 && ")
                else:
                    f.write(f"idx{i} == 1'b1")
            elif (comparator - 1) < i < (n_idx - 2):
                f.write(f"idx{i} == 1'b0 && ")
            elif i == (n_idx - 2):
                f.write(f"idx{i} == 1'b0")


def gen_top_file(sv_path, number_of_layers, number_of_classes,
                 num_neurons, outputs_per_class, output_bits, total_gates):
    """
    Generate top.sv — the overlay's top module.

    Changes from static version:
      - Adds clk, rst_n ports
      - Adds AXI-Lite slave interface (via axi_lut_ctrl)
      - Layer instantiations carry cfg_* ports
      - Inference I/O goes through axi_lut_ctrl registers
    """
    with open(os.path.join(sv_path, "top.sv"), "w") as f:
        f.write("// Auto-generated by generate_overlay.py\n")
        f.write("// Reconfigurable LLNN Overlay — Top Module\n\n")
        f.write("`include \"Globals.sv\"\n\n")

        f.write("module top (\n")
        f.write("\tinput  logic        clk,\n")
        f.write("\tinput  logic        rst_n,\n")
        f.write("\n")
        f.write("\t// AXI-Lite Slave (directly exposed for block design)\n")
        f.write("\tinput  logic [13:0] S_AXI_AWADDR,\n")
        f.write("\tinput  logic        S_AXI_AWVALID,\n")
        f.write("\toutput logic        S_AXI_AWREADY,\n")
        f.write("\tinput  logic [31:0] S_AXI_WDATA,\n")
        f.write("\tinput  logic [3:0]  S_AXI_WSTRB,\n")
        f.write("\tinput  logic        S_AXI_WVALID,\n")
        f.write("\toutput logic        S_AXI_WREADY,\n")
        f.write("\toutput logic [1:0]  S_AXI_BRESP,\n")
        f.write("\toutput logic        S_AXI_BVALID,\n")
        f.write("\tinput  logic        S_AXI_BREADY,\n")
        f.write("\tinput  logic [13:0] S_AXI_ARADDR,\n")
        f.write("\tinput  logic        S_AXI_ARVALID,\n")
        f.write("\toutput logic        S_AXI_ARREADY,\n")
        f.write("\toutput logic [31:0] S_AXI_RDATA,\n")
        f.write("\toutput logic [1:0]  S_AXI_RRESP,\n")
        f.write("\toutput logic        S_AXI_RVALID,\n")
        f.write("\tinput  logic        S_AXI_RREADY\n")
        f.write(");\n\n")

        # Internal wires
        f.write("\t// ---- Internal wires ----\n")
        f.write("\tlogic [NET_INPUTS-1:0] NET_I;\n")
        f.write(f"\tlogic [{output_bits-1}:0] NET_O;\n")
        f.write(f"\tlogic [GATE_SEL_W-1:0] cfg_gate_sel;\n")
        f.write(f"\tlogic cfg_ce, cfg_data;\n\n")

        for i in range(number_of_layers):
            f.write(f"\tlogic [L{i}_NEURONS-1:0] F_L{i};\n")

        c_width = "$clog2(CLASS_OUTS+1)"
        for i in range(number_of_classes):
            f.write(f"\tlogic [{c_width}-1:0] C{i};\n")
        for i in range(number_of_classes - 2):
            f.write(f"\tlogic [{c_width}-1:0] max{i};\n")
        for i in range(number_of_classes - 1):
            f.write(f"\tlogic idx{i};\n")
        f.write("\n")

        # AXI controller instance
        f.write("\t// ---- AXI Configuration Controller ----\n")
        f.write(f"\taxi_lut_ctrl #(\n")
        f.write(f"\t\t.TOTAL_GATES  (TOTAL_GATES),\n")
        f.write(f"\t\t.NET_INPUTS   (NET_INPUTS),\n")
        f.write(f"\t\t.NET_OUTPUTS  ({output_bits})\n")
        f.write(f"\t) u_axi_ctrl (\n")
        f.write(f"\t\t.S_AXI_ACLK    (clk),\n")
        f.write(f"\t\t.S_AXI_ARESETN (rst_n),\n")
        for sig in ["AWADDR", "AWVALID", "AWREADY",
                     "WDATA", "WSTRB", "WVALID", "WREADY",
                     "BRESP", "BVALID", "BREADY",
                     "ARADDR", "ARVALID", "ARREADY",
                     "RDATA", "RRESP", "RVALID", "RREADY"]:
            f.write(f"\t\t.S_AXI_{sig:10s} (S_AXI_{sig}),\n")
        f.write(f"\t\t.cfg_gate_sel  (cfg_gate_sel),\n")
        f.write(f"\t\t.cfg_ce        (cfg_ce),\n")
        f.write(f"\t\t.cfg_data      (cfg_data),\n")
        f.write(f"\t\t.net_i         (NET_I),\n")
        f.write(f"\t\t.net_o         (NET_O)\n")
        f.write(f"\t);\n\n")

        # Layer instances — with config ports
        f.write("\t// ---- Inference Layers ----\n")
        for i in range(number_of_layers):
            f.write(f"\tlayer{i} L{i} (\n")
            f.write(f"\t\t.clk          (clk),\n")
            if i == 0:
                f.write(f"\t\t.in           (NET_I),\n")
            else:
                f.write(f"\t\t.in           (F_L{i-1}),\n")
            f.write(f"\t\t.out          (F_L{i}),\n")
            f.write(f"\t\t.cfg_gate_sel (cfg_gate_sel),\n")
            f.write(f"\t\t.cfg_ce       (cfg_ce),\n")
            f.write(f"\t\t.cfg_data     (cfg_data)\n")
            f.write(f"\t);\n\n")

        # Popcount (unchanged)
        f.write("\t// ---- Popcount per class ----\n")
        f.write(f"\tfunction automatic [{c_width}-1:0] popcount;\n")
        f.write(f"\t\tinput [{outputs_per_class-1}:0] v;\n")
        f.write(f"\t\tinteger i;\n")
        f.write(f"\t\tbegin\n")
        f.write(f"\t\t\tpopcount = 0;\n")
        f.write(f"\t\t\tfor (i = 0; i < {outputs_per_class}; i = i + 1)\n")
        f.write(f"\t\t\t\tpopcount = popcount + v[i];\n")
        f.write(f"\t\tend\n")
        f.write(f"\tendfunction\n\n")

        for i in range(number_of_classes):
            hi = num_neurons[-1] - 1 - outputs_per_class * i
            lo = num_neurons[-1] - outputs_per_class * (i + 1)
            f.write(f"\tassign C{number_of_classes - i - 1} = "
                    f"popcount(F_L{number_of_layers-1}[{hi}:{lo}]);\n")
        f.write("\n")

        # Comparator chain (unchanged)
        f.write("\t// ---- Comparator reduction chain ----\n")
        for i in range(number_of_classes - 1):
            f.write(f"\tcomparator CMP{i} (\n")
            if i == 0:
                f.write(f"\t\t.in1 (C0),\n\t\t.in2 (C1),\n")
            else:
                f.write(f"\t\t.in1 (max{i-1}),\n\t\t.in2 (C{i+1}),\n")
            if i < number_of_classes - 2:
                f.write(f"\t\t.max (max{i}),\n")
            else:
                f.write(f"\t\t.max (),\n")
            f.write(f"\t\t.idx (idx{i})\n\t);\n\n")

        # Argmax output encoding (unchanged)
        f.write("\t// ---- Output encoding (argmax) ----\n")
        f.write("\talways_comb begin\n")
        f.write(f"\t\tNET_O = {output_bits}'b{'1' * output_bits};\n")
        for i in range(number_of_classes):
            binchars = f"{i:b}".zfill(output_bits)
            f.write("\t\tif (")
            print_idx(f, i, number_of_classes)
            f.write(f") NET_O = {output_bits}'b{binchars};\n")
        f.write("\tend\n\n")

        f.write("endmodule : top\n")


# =============================================================================
#  Wiring map export (for the PYNQ driver)
# =============================================================================

def export_wiring_map(sv_path, wiring_layers, num_neurons, lut_size,
                      number_of_inputs):
    """
    Export the gate-to-wiring mapping as JSON.
    The PYNQ driver uses this to know the address map.
    """
    gate_map = {"total_gates": sum(num_neurons), "net_inputs": number_of_inputs,
                "layers": []}
    gate_id = 0
    for l, indices in enumerate(wiring_layers):
        layer_info = {
            "layer": l,
            "num_neurons": num_neurons[l],
            "lut_size": lut_size[l],
            "gate_base": gate_id,
            "neurons": []
        }
        for n in range(num_neurons[l]):
            connections = [int(indices[i][n]) for i in range(lut_size[l])]
            layer_info["neurons"].append({
                "gate_id": gate_id,
                "inputs": connections
            })
            gate_id += 1
        gate_map["layers"].append(layer_info)

    map_path = os.path.join(sv_path, "wiring_map.json")
    with open(map_path, "w") as f:
        json.dump(gate_map, f, indent=2)
    print(f"[overlay] Wiring map: {map_path}")
    return gate_map


# =============================================================================
#  Copy static library modules
# =============================================================================

def copy_library_modules(sv_path):
    """Copy SoftLUT5.sv and axi_lut_ctrl.sv into the output directory."""
    lib_dir = os.path.join(os.path.dirname(__file__), "overlay")
    for mod in ["SoftLUT5.sv", "axi_lut_ctrl.sv"]:
        src = os.path.join(lib_dir, mod)
        dst = os.path.join(sv_path, mod)
        shutil.copy2(src, dst)
        print(f"[overlay] Copied {mod}")


# =============================================================================
#  Main orchestrator
# =============================================================================

def gen_overlay(model, name):
    """Generate the complete overlay HDL from a trained model."""
    # Extract model parameters
    number_of_layers, num_neurons, lut_size, number_of_inputs, number_of_classes = \
        get_model_params(model)

    total_gates = sum(num_neurons)
    output_bits = len(f'{number_of_classes - 1:b}')
    outputs_per_class = num_neurons[-1] // number_of_classes

    print(f"[overlay] Model: {name}")
    print(f"[overlay]   Layers: {number_of_layers}")
    print(f"[overlay]   Neurons/layer: {num_neurons}")
    print(f"[overlay]   LUT size/layer: {lut_size}")
    print(f"[overlay]   Inputs: {number_of_inputs}, Classes: {number_of_classes}")
    print(f"[overlay]   Total gates: {total_gates}")

    # Create output directory
    sv_path = os.path.join("data", "overlay", name)
    create_folder(sv_path)

    # Extract wiring (indices only — NO truth tables)
    wiring_layers = get_wiring(model, lut_size)

    # Generate HDL files
    gen_globals_file(sv_path, number_of_inputs, number_of_layers,
                     num_neurons, lut_size, outputs_per_class,
                     output_bits, total_gates)
    print(f"[overlay] Generated Globals.sv")

    gen_comparator_file(sv_path)
    print(f"[overlay] Generated comparator.sv")

    gen_top_file(sv_path, number_of_layers, number_of_classes,
                 num_neurons, outputs_per_class, output_bits, total_gates)
    print(f"[overlay] Generated top.sv")

    # Generate layer files with SoftLUT5 instances
    gate_base = 0
    for l, indices in enumerate(wiring_layers):
        if l == 0:
            input_width_param = "NET_INPUTS"
        else:
            input_width_param = f"L{l-1}_NEURONS"

        gen_layer_file(sv_path, l, indices, num_neurons[l],
                       lut_size[l], gate_base, total_gates,
                       input_width_param)
        print(f"[overlay] Generated layer{l}.sv  "
              f"({num_neurons[l]} neurons, gates {gate_base}–"
              f"{gate_base + num_neurons[l] - 1})")
        gate_base += num_neurons[l]

    # Copy library modules
    copy_library_modules(sv_path)

    # Export wiring map
    export_wiring_map(sv_path, wiring_layers, num_neurons, lut_size,
                      number_of_inputs)

    print(f"\n[overlay] ✅ Overlay HDL written to: {sv_path}/")
    return sv_path


# =============================================================================
#  Entry point
# =============================================================================

if __name__ == "__main__":
    args = get_args()
    if args.name is None:
        args.name = args.model

    model_path = Path("models") / f"{args.model}.pth"
    if not model_path.exists():
        print(f"[overlay] ERROR: Model file not found: {model_path}")
        sys.exit(1)

    model = torch.load(model_path, weights_only=False)
    gen_overlay(model, args.name)
